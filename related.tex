\section{Related Work}

The locomotion tasks that this paper foucses on involve changing between two poses and maintaining balance during the motion. Tasks that fall into this category and are extensively studied in robotics are sit-to-stand \cite{Faloutsos:2003,Iida:2004,Pchelkin:2010,Mistry:2010,Bahar:2014} and lie-to-stand \cite{morimoto:1998,Faloutsos:2001,Kanehiro:2007,Hirukawa:2005}. While many of these prior work focuses on one specific motion, we target at a more general problem of finding controllers for a wide range of such locomotion tasks. A few related work tried to tackle this more general problem. Jones \cite{jones:2011} developed rising motions for both biped and quadraped using pose tracking, orientation correction and virtual force. Lin and Huang \cite{lin:2012} used motion planning and dynamics filtering to develop rising up motions from various initial lying poses. Tassa et al. \cite{tassa:2012} used Model Predictive Control to synthesize complex behaviors, including getting up from an aribitrary pose on the ground. Although these work has shown impressive results in simulation, experiments on real robots were not presented.

Locomotion often involve frequent change of contacts. It poses significant challenges to controller optimization due to the discontinuous contact forces. We choose to use Covariance Matrix Adaptation (CMA) \cite{Hansen:2009}, a stochastic sampling-based optimization algorithm, to tackle this challenge. Although CMA is not widely used in robotics, it is a popular method in physically-based character animation to search for control parameters when the problem domain is highly discontinous \cite{Wu:2010, Wang:2010, Tan:2014}.

A controller that is designed in simulation may not work in the real environment. This discrepancy of performance is called Reality Gap. One way to cross the reality gap is to improve the simulation model using real data that is measured from robot experiments. Ha and Yamane \cite{HA:2015} performed Gaussian process regression to model the error between the simulated and the real data, and then augmented the simulation with this error model. Abbeel et al. \cite{Abbeel:2006} started from an inaccurate physical model but successively grounded the policy evaluations using real-life trials. Grounded simulated learning approach \cite{Farchy:2013} iteratively optimized the controller, measured the discrepancy and modified the simulator using supervised learning algorithms. Zagal et al. \cite{zagal2004} coevolved the controller and the simulator using genetic algorithms.

Our simulation calibration subsystem narrows down the Reality Gap using off-white dynamic system identification methods \cite{ljung:2010}. A typical dynamic system identification involve experiment design and parameter estimation \cite{swevers:2007}. Experiments could be designed manually or automatically with genetic algorithms \cite{BongardL05}. In locomotion and manipulation related tasks, in which contact changes are frequent, the experiments are usually designed independently from control tasks because the parameters are better inferred with a contact free behavior \cite{Kolev:2015}. As a result, lengthy experiments and huge amount of data are needed in system identification, hoping that the collected data covers the important regions of the control space. In contrast, our system tightly couples simulation calibration and controller optimization. Using CMA in simulation calibration, we are able to estimate parameters even when contact changes in the experiments. 

