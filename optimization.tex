\section{Controller Optimization}
\ignorethis{
\karen{If the goal is to create robust controllers, can we later use the calibrated simulation parameters to create a feedback controller (\eg using inverted pendulum model) via simulator?}
\jie{I hope so but I do not know. Since our experiments only cover open-loop control, it might not be necessary to speculate whether our system will work on feedback controller in the paper. But I still mention that extending the current system to feedback controllers is one of the future wok.}
\karen{I agree that this should be considered as future work. I belive that matching simulation and reality will be much more difficult for a feedback controller. I think of our algorithm as to match stability region of simulation with that of hardware. For open-loop controller, we need to match two points, but for feedback controller we need to match two manifolds.}}

Given the physical simulation, we can design controllers to enable the robot to achieve various locomotion tasks in the simulated environment. The four tasks that we use to test our system are rising from a sitting, leaning or kneeling position and flipping towards a handstand position (Figure \ref{fig:sit2Stand}, \ref{fig:lean2Stand}, \ref{fig:kneel2Stand} and \ref{fig:stand2Hand}). For each task, the joint configuration of the initial pose and the final pose are provided by the user. The goal of controller optimization is to find a sequence of control signals $\bar{\mathbf{q}}(t)$ so that the robot can move from the initial to the final pose without losing balance. We purposefully choose to use only open-loop controllers\footnote{The internal feedback loop in the actuators still exist but we do not alter this feedback loop in our controller design.} in this work, which means that the control signal $\bar{\mathbf{q}}(t)$ is a function of time $t$ and does not depend on the states of the robot. Using only the open-loop controller can better evaluate our system, especially simulation calibration, because the accuracy of the simulation becomes more critical if feedback control is not allowed.

We first formulate a trajectory optimization problem for each task.
\begin{align}
 \label{eqn:obj}&\max_{\bar{\mathbf{q}}(t),T} V_{ctrl}(\mathbf{x}(\bar{\mathbf{q}}(t)))\\
\nonumber  \mathrm{subject\;} &\mathrm{to} \\
\label{eqn:dyn1} & \mathbf{M}(\mathbf{x})\mathbf{\ddot{x}}+\mathbf{C}(\mathbf{x},\mathbf{\dot{x}}) =\boldsymbol{\tau}(\mathbf{q}, \dot{\mathbf{q}}, \bar{\mathbf{q}}) + \mathbf{J}^T\mathbf{f}\\
\label{eqn:boundary1}&\bar{\mathbf{x}}(0) = \mathbf{x}_0\\
\label{eqn:boundary2}&\bar{\mathbf{q}}(t) = \mathbf{q}_T, \text{if } t \geq T
\end{align}

\ignorethis{\karen{At this point, it is not clear how we get \vc{q} and \vc{f} in the optimization.}
\jie{Let's wait for Byron and see his opinion on this.}}
This optimization searches for the duration $T$ of the entire motion and the trajectory of the desired joint configuration $\bar{\mathbf{q}}(t)$ to maximize a task-related fitness function $V_{ctrl}$, and subject to physical constraints (eq.(\ref{eqn:dyn1})) and boundary conditions (eq.(\ref{eqn:boundary1}) and (\ref{eqn:boundary2})). $\mathbf{x}_0$ is the initial condition, and $\mathbf{q}_T$ is the final pose. Note that although we can specify the full state in the initial condition, we can only specify the desired joint angles for the final pose because the global translation and rotation are determined by the physical simulation.

We parameterize the desired trajectory $\bar{\mathbf{q}}(t)$ to reduce the dimension of the optimization problem. Although there are many ways that we can parameterize the control space, designing the most effective control parametrization is not the focus of this work. In this work, we choose a simple parametrization that use a sparse set of keyframes $\bar{\mathbf{q}}_1, \bar{\mathbf{q}}_2, ..., \bar{\mathbf{q}}_n$. Between the keyframes, we linearly interpolate the poses from two adjacent keyframes. With this simplification, the \emph{control parameters} that we need to optimize reduce to only a few keyframes and the time intervals between adjacent keyframes.

\ignorethis{
\karen{Why do we have to assume no inter-body collision? There are many other possible ways that the desired trajectoires achieve the objective in an undesired way (\eg jump of the ground), do we also need to assume that those situations won't happen? More fundamental question: how can we make any of these assumptions while our optimizer doesn't know about these assumptions?}
\jie{I removed the first few sentences that confuse you. Actually we do not assume anything. If the robot jumps off the ground in the simulation, we do not do anything to prevent that. You can see this from the video of stand-to-handstand. Before simulation calibration, the optimal controller indeed jumps in the simulation. But after calibration, the optimal controller does not jump in the simulation, which matches the real world performance. In term of self-collision, it did not happen because we chose a reasonable range of joint angles to search in CMA.}
\karen{That sounds fine.}
}

The criterion of success for all the tasks is whether the robot remains upright at the end of its motion. We use the following fitness function to reward controllers that keep balance throughout the entire motion.


\begin{equation}
  V_{ctrl}(\mathbf{x}(\bar{\mathbf{q}}(t)))=\int_0^{T+1} \frac{1}{\alpha(\bar{\mathbf{q}}(t))+\epsilon}\mathrm{d}t
  \label{eqn:controllerObj}
\end{equation}
where $\alpha$ is the angle between the up direction in the local frame of the robot's torso and the up direction in the global frame $(0,0,1)$. It measures how far the robot is from losing its balance. $\epsilon$ is a small positive number to prevent the denominator from being zero. We choose $\epsilon=0.1$ in all our tasks. Note that the upper limit of the integration is $T+1$. The extra one second is to wait for the robot to settle down. We use the time horizon $T+1$ because it is still possible that the robot can fall during the settling down phase and our fitness function will penalize this situation.

During locomotion, discrete contact events can happen frequently. They impose additional challenges for the continuous optimization algorithms that rely on gradient information. To overcome this challenge, we choose to use CMA to optimize the control parameters. CMA is a sample-based stochastic optimization algorithm, which has a solid theoretical foundation \cite{akimoto:2010,glasmachers:2010}. It does not need gradient information and can explore multiple local minima. This makes it ideal to optimize controllers for locomotion tasks. For the completeness of the paper, we will briefly describe the CMA algorithm here. Please refer to the original paper \cite{Hansen:2009} for more details. At each iteration, a population of CMA samples are drawn from an underlying Gaussian distribution. In our case, a CMA sample is a candidate controller. Each CMA sample is then simulated and evaluated using eq.(\ref{eqn:controllerObj}). The samples with lower fitness values are discarded. The underlying Gaussian distribution is updated according to the remaining good samples. With more iterations, the distribution gradually converges to a better region of the control space. After a maximum number of iterations, we choose the best CMA sample as the output of the controller optimization subsystem.
