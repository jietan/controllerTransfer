\section{Trajectory Optimization}

Given the physical simulation, we can design controllers to enable the robot to achieve various transition tasks in the simulated environment. The four tasks that we use to test our system are rising from a sitting, leaning or kneeling position and flipping towards a handstand position (Figure \ref{fig:sit2Stand}, \ref{fig:lean2Stand}, \ref{fig:kneel2Stand} and \ref{fig:stand2Hand}). For each task, the initial pose $\bar{\mathbf{q}}(0)$ and the final pose $\bar{\mathbf{q}}(T)$ are given as the input. The goal of this optimization is to find a reference trajectory $\bar{\mathbf{q}}(t)$ so that the robot can move from the initial to the final pose without losing balance. We purposefully choose to control the robot with only open-loop reference trajectories\footnote{The internal feedback loop in the actuators still exist but we do not alter this feedback loop in our trajectory optimization.} in this work, which means that the control signal $\bar{\mathbf{q}}(t)$ is a function of time $t$ and does not depend on the states of the robot. Using only the open-loop reference trajectory can better evaluate our system, especially simulation calibration, because the accuracy of the simulation becomes more critical if no feedback balance control is involved.

\subsection{Trajectory Parameterization}
We parameterize the reference trajectory $\bar{\mathbf{q}}(t)$ with a sparse set of keyframes $\{\bar{\mathbf{q}}_i\}_{i=1...n}$ at time $\{t_i\}_{i=1...n}$. The reference trajectory is linearly interpolated between the keyframes. The optimization searches for both the joint configuration and the time of each keyframe. In our implementation, the number of keyframes $n$ is fixed and manually specified by the user, although it is possible to incorporate it in an outer-loop optimization.


\subsection{Fitness Function}
The criterion of success for all the tasks is whether the robot remains upright (or upside down in the last task) at the end of its motion. We use the following fitness function to reward the trajectories that keep the robot balanced throughout the entire motion.

\begin{equation}
  \{\bar{\mathbf{q}}_i\}_{i=1...n}, \{{t}_i\}_{i=1...n}=\argmax\int_0^{T+1} \frac{1}{|\alpha(t)|+\epsilon}\mathrm{d}t
  \label{eqn:controllerObj}
\end{equation}
where $\alpha$ is the angle between the up direction in the local frame of the robot's torso and the up (or down in the last task) direction in the global frame. It measures how far the robot is from losing its balance. $\epsilon$ is a small positive number to prevent the denominator from being zero. We choose $\epsilon=0.1$ in all our tasks. $T$ is the time when the robot reaches the final pose. Note that the upper limit of the integration is $T+1$. The extra one second is to wait for the robot to settle down. We use the time horizon $T+1$ because it is still possible that the robot can fall during the settling down phase and our fitness function will penalize this situation.

\subsection{Optimization with CMA}
During the transition motion, discrete contact events can happen frequently. They impose additional challenges for the continuous optimization algorithms that rely on gradient information. To overcome this challenge, we choose to use CMA to optimize the reference trajectory. CMA is a sample-based stochastic optimization algorithm, which has a solid theoretical foundation \cite{akimoto:2010,glasmachers:2010}. It does not need gradient information and can explore multiple local minima. This makes it ideal to optimize open loop trajectories or feedback controllers for transition tasks. For the completeness of the paper, we will briefly describe the CMA algorithm here. Please refer to the original paper \cite{Hansen:2009} for more details. At each iteration, a population of CMA samples are drawn from an underlying Gaussian distribution. In our case, a CMA sample is a set of keyframes and their associated time. Each CMA sample is used to control the robot in the simulation and evaluated using eq.(\ref{eqn:controllerObj}). The samples with lower fitness values are discarded. The underlying Gaussian distribution is updated according to the remaining good samples. With more iterations, the distribution gradually converges to a better region of the search space. After a maximum number of iterations, we choose the best CMA sample and reconstruct the optimal reference trajectory $\bar{\mathbf{q}}(t)$.
